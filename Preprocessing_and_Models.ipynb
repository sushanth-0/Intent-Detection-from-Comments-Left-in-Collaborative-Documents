{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44j6r5BeZdMU",
        "outputId": "7d5a4550-32a4-4b74-82a9-e366044510eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX8zBHVdjUfV",
        "outputId": "c8a515c3-e852-4573-c5ad-32e23f7a513f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.17.2)\n",
            "Requirement already satisfied: torchaudio in ./.local/lib/python3.9/site-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.9/site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: sympy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (1.10.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: jinja2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.9/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.9/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: networkx in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (2.7.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: numpy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (0.28.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyyaml in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: psutil in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.9/site-packages (from accelerate) (0.22.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
            "Requirement already satisfied: sympy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.6.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: jinja2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: networkx in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.64.0)\n",
            "Requirement already satisfied: requests in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install accelerate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrfvzHtF8ZMM",
        "outputId": "c94edf31-2af8-47c8-c99a-790478cdab6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (0.28.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.9/site-packages (from accelerate) (2.2.2)\n",
            "Requirement already satisfied: psutil in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (5.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (1.21.5)\n",
            "Requirement already satisfied: pyyaml in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.9/site-packages (from accelerate) (0.22.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from accelerate) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from packaging>=20.0->accelerate) (3.0.4)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: jinja2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: networkx in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (2.7.1)\n",
            "Requirement already satisfied: sympy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch>=1.10.0->accelerate) (1.10.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub->accelerate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from huggingface-hub->accelerate) (4.64.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate) (1.26.9)\n",
            "Requirement already satisfied: mpmath>=0.19 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQgS7zSiaIPe"
      },
      "outputs": [],
      "source": [
        "# Install the transformers library if not already available\n",
        "#!pip install transformers\n",
        "\n",
        "# Now, import the required modules\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ev0A1GetCFHL",
        "outputId": "b5824826-ef5d-4208-d65c-0726bb8a37bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.2.2)\n",
            "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (0.17.2)\n",
            "Requirement already satisfied: torchaudio in ./.local/lib/python3.9/site-packages (2.2.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.9/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (3.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: jinja2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (2.11.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.9/site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.9/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: networkx in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (2.7.1)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: fsspec in ./.local/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: sympy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torch) (1.10.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torchvision) (9.0.1)\n",
            "Requirement already satisfied: numpy in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from jinja2->torch) (2.0.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from sympy->torch) (1.2.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.39.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
            "Requirement already satisfied: tqdm>=4.27 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.9/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /software/spackages/linux-rocky8-x86_64/gcc-9.5.0/anaconda3-2022.05-zyrazrj6uvrtukupqzhaslr63w7hj6in/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --upgrade\n",
        "!pip install transformers --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1VhFRfSaZvl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Update the path below to the path of your dataset in Google Drive\n",
        "#file_path = '/content/drive/My Drive/ADSproject/data/labeled_comments_cleaned.csv'\n",
        "df = pd.read_csv('labeled_comments_cleaned.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8IF9pbob2vI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Function to clean text data\n",
        "def clean_text(text):\n",
        "    # Remove symbols and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Clean the comment_full_text column\n",
        "df['comment_full_text'] = df['comment_full_text'].apply(clean_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN9HiBgncE8x"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Selecting the feature and target variable\n",
        "X = df['comment_full_text']\n",
        "y = df['level_0']\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340,
          "referenced_widgets": [
            "8c2ac8f170934c02a1852ac13930faba",
            "bb707c5f94e04e55b64641b29a29e58f",
            "d56a35c9aee947c49f6ed48f441cebaa",
            "8f8ccbfe8afd45d7ab45dc7607d9fbc6",
            "99b6ec6c90db4f95a03289042e7480bd"
          ]
        },
        "id": "rZEKaNR9fMhQ",
        "outputId": "f24de1d8-e9e8-4d22-8377-da22c45c6da2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c2ac8f170934c02a1852ac13930faba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb707c5f94e04e55b64641b29a29e58f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d56a35c9aee947c49f6ed48f441cebaa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f8ccbfe8afd45d7ab45dc7607d9fbc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99b6ec6c90db4f95a03289042e7480bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Load the DistilBERT model pre-trained for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4) # Adjust num_labels based on your intents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WUWIl-ofTtM"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JloVcXU1hhY-"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Convert the y labels to integers\n",
        "label_dict = {v: k for k, v in enumerate(df['level_0'].unique())}\n",
        "y_train_int = y_train.replace(label_dict).values\n",
        "y_test_int = y_test.replace(label_dict).values\n",
        "\n",
        "# Prepare the dataset\n",
        "train_dataset = Dataset(train_encodings, y_train_int)\n",
        "test_dataset = Dataset(test_encodings, y_test_int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ-R15oQhkT-",
        "outputId": "ed079118-54bb-417d-870c-ebc15da85dbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model and logs\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    learning_rate=2e-5, # you can adjust this learning rate\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset            # evaluation dataset\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HdbZQ822h2ZJ",
        "outputId": "f064df0a-9daf-4090-cf3e-e2bf4eb6665b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='390' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [390/750 5:43:54 < 5:19:05, 0.02 it/s, Epoch 1.56/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.388000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.382500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.357900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.274300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.052300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.974200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.896600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.838700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.895500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.797500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.740500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.689500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.741800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.617700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='460' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [460/750 6:45:06 < 4:16:30, 0.02 it/s, Epoch 1.84/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.388000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.382500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.375400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.357900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.274300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.159900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.052300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.032100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.974200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.896600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.870900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.838700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.895500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.797500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.780300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.730000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.720800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.740500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.689500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.741800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.617700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.705300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.660800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.627400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.608100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.567600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.544300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "n8Dg9Jh44Nnl",
        "outputId": "dfa9f782-0f62-4464-99a0-f4ff5cfa1d4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/vpasam/.local/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='498' max='498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [498/498 01:37, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.415300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.404200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.399800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.377700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.329500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.236400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.165900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.023400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.055200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.928500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.975200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.899600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.858500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.846700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.900800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.794300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.790800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.708600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.772000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.703100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.670800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.737600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.587000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.657100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.682000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.585600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.673800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.734400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.546000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.655600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.569100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.620000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.695600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.602100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.486400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='498' max='498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [498/498 01:37, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.447900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.608100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.411000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.536200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.408200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.539700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.494000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.527200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.435300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.544700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.421600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.570800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.454100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.421800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.491200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.484100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.439100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.371600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.533800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.421400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.414800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.417900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.517300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.390300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.491200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.342800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.462400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.557600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.469200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.374100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.354100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.440900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.480400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.367900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Arguments for training\n",
        "training_args = TrainingArguments(\n",
        "    # ... other arguments ...\n",
        "    output_dir='./results',          # output directory for model and logs\n",
        "    num_train_epochs=2,              # total number of training epochs\n",
        "    learning_rate=2e-5, # you can adjust this learning rate\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients\n",
        "    # fp16=True,  # Enable mixed precision training\n",
        "    save_steps=200,  # Save model every 200 steps\n",
        "    save_total_limit=2,  # Only keep the last two checkpoints\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Training in a loop to save intermittently\n",
        "for epoch in range(training_args.num_train_epochs):\n",
        "    # trainer.train(resume_from_checkpoint=True)\n",
        "    trainer.train()\n",
        "    trainer.save_model(f\"./model_checkpoint_epoch_{epoch}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOcoaew_rJ3d",
        "outputId": "a89a35ea-a4e7-42b9-aebe-a1669bfa686e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6278952956199646,\n",
              " 'eval_runtime': 3.6471,\n",
              " 'eval_samples_per_second': 273.917,\n",
              " 'eval_steps_per_second': 17.274,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3gA8jQxrJ3e",
        "outputId": "411f60f7-3761-4e07-cac5-f6137e62e94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error Rate: 0.228\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming predictions are made using the Trainer's predict method on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Get the predicted labels by taking the argmax of the logits from the predictions\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Calculate accuracy using the true labels and the predicted labels\n",
        "accuracy = accuracy_score(predictions.label_ids, predicted_labels)\n",
        "\n",
        "# The test error rate is 1 minus the accuracy\n",
        "test_error_rate = 1 - accuracy\n",
        "\n",
        "print(f\"Test Error Rate: {test_error_rate:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlZJZz-SrJ3e",
        "outputId": "0f248b12-a465-46b1-bfa3-99da77827686"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'Can we include the latest survey data in our analysis?'\n",
            "Predicted intent: LABEL_1\n",
            "\n",
            "Text: 'Great teamwork on the recent launch, everyone should be proud!'\n",
            "Predicted intent: LABEL_1\n",
            "\n",
            "Text: 'Where can I find the financial report for Q2?'\n",
            "Predicted intent: LABEL_1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Make sure to load the model and tokenizer\n",
        "model_name = \"distilbert-base-uncased\"  # Replace with your model name if different\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move the model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Example sentences for prediction\n",
        "sentences = [\n",
        "    \"Can we include the latest survey data in our analysis?\",\n",
        "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
        "    \"Where can I find the financial report for Q2?\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "encoded_inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move the tokenized inputs to the device\n",
        "input_ids = encoded_inputs[\"input_ids\"].to(device)\n",
        "attention_mask = encoded_inputs[\"attention_mask\"].to(device)\n",
        "\n",
        "# Predict\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Get the predictions\n",
        "logits = outputs.logits\n",
        "predictions = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Move predictions back to CPU for further operations if needed\n",
        "predictions = predictions.detach().cpu().numpy()\n",
        "\n",
        "# Convert the predictions to label names if you have a mapping\n",
        "# If using a model from Hugging Face with an associated config, you can convert the indices to labels directly\n",
        "label_names = [model.config.id2label[pred] for pred in predictions]\n",
        "\n",
        "# Output the predictions\n",
        "for sentence, label in zip(sentences, label_names):\n",
        "    print(f\"Text: '{sentence}'\\nPredicted intent: {label}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnXGZRLVrJ3f",
        "outputId": "8b18a6bc-2333-4053-8146-d2167247213e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/vpasam/.local/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [750/750 02:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.365300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.293700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.243800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.186000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.069300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.967000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.880500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.921700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.882200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.749900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.749200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.694000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.731900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.706900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.609800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.507700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.508900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.606100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.498600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.613800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.587700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.661800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.442100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.550800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.505400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.423500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.802100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.514900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.464300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.474400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.433500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.580700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.461900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.405600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.461000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.413200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.426800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.449000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.396400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.475500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.459300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.440700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.351400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.333200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.435100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.372900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.350300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.421900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.325200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: \"Can we include the latest survey data in our analysis?\"\n",
            "Predicted intent: Information Exchange\n",
            "\n",
            "Text: \"Great teamwork on the recent launch, everyone should be proud!\"\n",
            "Predicted intent: Social Communication\n",
            "\n",
            "Text: \"Where can I find the financial report for Q2?\"\n",
            "Predicted intent: Information Exchange\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Initialize the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4) # Replace NUM_LABELS with the actual number of labels in your task\n",
        "\n",
        "# Tokenize the input texts\n",
        "texts = [\n",
        "    \"Can we include the latest survey data in our analysis?\",\n",
        "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
        "    \"Where can I find the financial report for Q2?\"\n",
        "]\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move the model and inputs to the GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model and logs\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # replace with your actual training dataset\n",
        "    eval_dataset=test_dataset,    # replace with your actual evaluation dataset\n",
        ")\n",
        "\n",
        "# Train and evaluate\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "# Make predictions\n",
        "model.eval()  # Put the model in evaluation mode\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "# Convert logits to class labels\n",
        "predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# Map the predictions to actual labels (you need to replace LABEL_MAPPING with your actual label mapping)\n",
        "LABEL_MAPPING = {0: \"Information Exchange\", 1: \"Modification\", 2: \"Social Communication\", 3: \"Other\"}  # Replace with actual mapping\n",
        "predicted_labels = [LABEL_MAPPING[label] for label in predictions]\n",
        "\n",
        "# Print the predictions\n",
        "for text, label in zip(texts, predicted_labels):\n",
        "    print(f'Text: \"{text}\"\\nPredicted intent: {label}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JPKTL5prJ3f",
        "outputId": "d4ccb614-da69-4f63-d1fd-f02163c7dbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error Rate: 0.222\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming predictions are made using the Trainer's predict method on the test dataset\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Get the predicted labels by taking the argmax of the logits from the predictions\n",
        "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Calculate accuracy using the true labels and the predicted labels\n",
        "accuracy = accuracy_score(predictions.label_ids, predicted_labels)\n",
        "\n",
        "# The test error rate is 1 minus the accuracy\n",
        "test_error_rate = 1 - accuracy\n",
        "\n",
        "print(f\"Test Error Rate: {test_error_rate:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URDLEuUvrJ3f",
        "outputId": "d1e312c4-66d6-4c1c-d011-57d8a4314760"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: \"Is there an update on the project timeline?\"\n",
            "Predicted intent: Modification\n",
            "\n",
            "Text: \"Congratulations on reaching your sales targets!\"\n",
            "Predicted intent: Modification\n",
            "\n",
            "Text: \"Could you provide me with the meeting minutes?\"\n",
            "Predicted intent: Modification\n",
            "\n",
            "Text: \"Let's schedule a review meeting for Monday.\"\n",
            "Predicted intent: Modification\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Texts to predict\n",
        "texts_to_predict = [\n",
        "    \"Is there an update on the project timeline?\",\n",
        "    \"Congratulations on reaching your sales targets!\",\n",
        "    \"Could you provide me with the meeting minutes?\",\n",
        "    \"Let's schedule a review meeting for Monday.\"\n",
        "]\n",
        "\n",
        "# Tokenize the texts\n",
        "inputs = tokenizer(texts_to_predict, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Move the inputs to the same device as the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Initialize the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)  # Replace num_labels with your actual number\n",
        "model.to(device)\n",
        "\n",
        "# Get the model's predictions\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Map predictions to the actual labels\n",
        "# Replace this with your actual label mapping\n",
        "label_mapping = {0: 'Information Exchange', 1: 'Modification', 2: 'Social Communication', 3: 'Other'}\n",
        "predicted_labels = [label_mapping[pred.item()] for pred in predictions]\n",
        "\n",
        "# Display predictions\n",
        "for text, label in zip(texts_to_predict, predicted_labels):\n",
        "    print(f'Text: \"{text}\"\\nPredicted intent: {label}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J7khtw-rJ3g",
        "outputId": "59247b03-05ae-4006-802b-ee8bb1e096f7",
        "colab": {
          "referenced_widgets": [
            "01906ee9797040a48a4dc90b602ad301",
            "ec581cbda6c64c8d98534ee560763132"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01906ee9797040a48a4dc90b602ad301",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4491 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec581cbda6c64c8d98534ee560763132",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/vpasam/.local/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='843' max='843' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [843/843 02:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.580863</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.795970</td>\n",
              "      <td>0.803680</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.735200</td>\n",
              "      <td>0.527431</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.802169</td>\n",
              "      <td>0.810429</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.735200</td>\n",
              "      <td>0.542536</td>\n",
              "      <td>0.792000</td>\n",
              "      <td>0.791609</td>\n",
              "      <td>0.795230</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "('./trained_model/tokenizer_config.json',\n",
              " './trained_model/special_tokens_map.json',\n",
              " './trained_model/vocab.txt',\n",
              " './trained_model/added_tokens.json',\n",
              " './trained_model/tokenizer.json')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Install the required packages if running in a Google Colab environment\n",
        "# !pip install transformers datasets sklearn\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "# Define a function to compute metrics for evaluation\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('labeled_comments_cleaned.csv')  # Replace with your file path\n",
        "\n",
        "# Preprocess your data if necessary (e.g., you might want to clean text data)\n",
        "\n",
        "# Map labels to integers\n",
        "label_list = data['level_0'].unique().tolist()\n",
        "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "\n",
        "# Split the data\n",
        "train_data, val_data = train_test_split(data, test_size=0.1)\n",
        "train_data['labels'] = train_data['level_0'].map(label_to_id)\n",
        "val_data['labels'] = val_data['level_0'].map(label_to_id)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the text\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['comment_full_text'], padding=True, truncation=True)\n",
        "\n",
        "# Create Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "val_dataset = Dataset.from_pandas(val_data)\n",
        "\n",
        "# Tokenize datasets\n",
        "train_dataset = train_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
        "val_dataset = val_dataset.map(tokenize, batched=True, remove_columns=['comment_full_text', 'level_0'])\n",
        "\n",
        "# Convert datasets to the Torch format\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "\n",
        "# Load the model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    'distilbert-base-uncased',\n",
        "    num_labels=len(label_list)\n",
        ")\n",
        "\n",
        "# Define training args\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "model_path = \"./trained_model\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNvoOL4ZrJ3g",
        "outputId": "9fac73f1-91c2-49ad-9755-71ae562007af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['INFORMATION EXCHANGE', 'SOCIAL COMMUNICATION', 'INFORMATION EXCHANGE']\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load trained model and tokenizer\n",
        "model_path = './trained_model'\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Function to make predictions\n",
        "def predict(texts):\n",
        "    # Tokenize the texts\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Move tensor to the right device\n",
        "    encoded_input.to(model.device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input)\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probabilities = torch.nn.functional.softmax(output.logits, dim=-1)\n",
        "\n",
        "    # Get the predicted class indices\n",
        "    predictions = torch.argmax(probabilities, dim=-1).cpu().numpy()\n",
        "\n",
        "    # Convert predicted class indices to labels\n",
        "    predicted_labels = [id_to_label[idx] for idx in predictions]\n",
        "    return predicted_labels\n",
        "\n",
        "# Example usage:\n",
        "texts = [\n",
        "    \"Can we include the latest survey data in our analysis?\",\n",
        "    \"Great teamwork on the recent launch, everyone should be proud!\",\n",
        "    \"Where can I find the financial report for Q2?\"\n",
        "]\n",
        "\n",
        "predicted_labels = predict(texts)\n",
        "print(predicted_labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDJhXaWrrJ3g",
        "outputId": "cdb7fbe2-5ec6-4fbc-8514-e4da0287cadb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: 'Is there an update on the project timeline?'\n",
            "Predicted intent: Information Exchange\n",
            "Text: 'Congratulations on reaching your sales targets!'\n",
            "Predicted intent: Social Communication\n",
            "Text: 'Could you provide me with the meeting minutes?'\n",
            "Predicted intent: Information Exchange\n",
            "Text: 'Let's schedule a review meeting for Monday.'\n",
            "Predicted intent: Modification\n"
          ]
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Make sure to replace 'path_to_your_model' with the actual path where your model is saved\n",
        "model_path = './trained_model'\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Put your model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Define your new examples here\n",
        "examples = [\n",
        "    \"Is there an update on the project timeline?\",\n",
        "    \"Congratulations on reaching your sales targets!\",\n",
        "    \"Could you provide me with the meeting minutes?\",\n",
        "    \"Let's schedule a review meeting for Monday.\"\n",
        "]\n",
        "\n",
        "# Tokenize and predict the intents of the new examples\n",
        "for example in examples:\n",
        "    inputs = tokenizer(example, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_label = logits.argmax(dim=-1).item()\n",
        "\n",
        "    # Convert the predicted label index to the actual label name, replace this with your actual label names\n",
        "    label_names = ['Information Exchange', 'Modification', 'Social Communication', 'Other']\n",
        "    predicted_intent = label_names[predicted_label]\n",
        "\n",
        "    print(f\"Text: '{example}'\")\n",
        "    print(f\"Predicted intent: {predicted_intent}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZwnOZOJrJ3h",
        "outputId": "ee53b841-5de8-47e1-f5bf-40750afbe483"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/vpasam/.local/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='624' max='624' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [624/624 00:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'test_loss': 1.393572449684143, 'test_accuracy': 0.180324584251653, 'test_f1': 0.18525376288504034, 'test_precision': 0.545282334316204, 'test_recall': 0.180324584251653, 'test_runtime': 21.4882, 'test_samples_per_second': 232.267, 'test_steps_per_second': 29.039}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('./trained_model/tokenizer_config.json',\n",
              " './trained_model/special_tokens_map.json',\n",
              " './trained_model/vocab.txt',\n",
              " './trained_model/added_tokens.json')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import DistilBertTokenizer, Trainer, TrainingArguments, DistilBertForSequenceClassification\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "test_dataframe = pd.read_csv('labeled_comments_cleaned.csv')\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the test dataset\n",
        "test_encodings = tokenizer(test_dataframe['comment_full_text'].tolist(), truncation=True, padding='max_length', max_length=512)\n",
        "\n",
        "# Correct the label_to_id dictionary to match the case used in your dataframe\n",
        "label_to_id = {\n",
        "    'Information Exchange': 0,\n",
        "    'Modification': 1,\n",
        "    'Social Communication': 2,\n",
        "    'Other': 3\n",
        "}\n",
        "\n",
        "# Ensure your dataframe 'level_0' column has labels that match the keys in label_to_id\n",
        "# If not, you need to convert them to match. Here's how you can do it:\n",
        "test_dataframe['level_0'] = test_dataframe['level_0'].str.title()  # Convert to title case to match keys\n",
        "\n",
        "# Convert the list of labels to the correct format, mapping each label to its corresponding ID\n",
        "true_labels = [label_to_id[label] for label in test_dataframe['level_0']]\n",
        "\n",
        "# Create a test dataset\n",
        "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': true_labels})\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_to_id))\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    do_predict=True\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "\n",
        "# Convert the predictions to labels\n",
        "pred_labels = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Calculate metrics\n",
        "def compute_metrics(p):\n",
        "    pred_labels = np.argmax(p.predictions, axis=-1)\n",
        "    accuracy = accuracy_score(p.label_ids, pred_labels)\n",
        "    f1 = f1_score(p.label_ids, pred_labels, average='weighted')\n",
        "    precision = precision_score(p.label_ids, pred_labels, average='weighted')\n",
        "    recall = recall_score(p.label_ids, pred_labels, average='weighted')\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "# Initialize the Trainer with compute_metrics\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,  # Assuming train_dataset is available\n",
        "    eval_dataset=val_dataset,  # Assuming val_dataset is available\n",
        "    compute_metrics=compute_metrics  # Pass the compute_metrics function here\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")\n",
        "print(eval_results)\n",
        "\n",
        "# Remember to save the trained model and tokenizer if needed\n",
        "model_path = \"./trained_model\"\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-UJ2pkorJ3h",
        "outputId": "5e3faee0-c4e0-4507-f570-f20581a14a4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.18\n",
            "Precision: 0.55\n",
            "Recall: 0.18\n",
            "F1 Score: 0.19\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `true_labels` and `pred_labels` are your actual and predicted labels respectively\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, pred_labels)\n",
        "precision = precision_score(true_labels, pred_labels, average='weighted')  # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
        "recall = recall_score(true_labels, pred_labels, average='weighted')        # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
        "f1 = f1_score(true_labels, pred_labels, average='weighted')                # Use 'micro', 'macro', 'weighted', or 'binary' for binary classification\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "# Generate and visualize the confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_names, yticklabels=label_names)\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLR31IzDrJ3h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}